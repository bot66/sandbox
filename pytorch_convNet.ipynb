{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minist dataset: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST(\"./data/\", train=True, download=True) #60000\n",
    "test= MNIST(\"./data/\",train=False, download=True) #10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  4\n",
      "size:  (28, 28)\n",
      "max pixle value:  255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show random train sample\n",
    "num = np.random.randint(len(train))\n",
    "\n",
    "plt.imshow(train[num][0], cmap='gray')\n",
    "print(\"label: \", train[num][1])\n",
    "\n",
    "print(\"size: \", train[num][0].size)\n",
    "\n",
    "print(\"max pixle value: \", np.max(train[num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define net\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "    \n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        # forward process\n",
    "#         print(\"input shape: \", x.shape)\n",
    "        x = self.conv1(x)\n",
    "#         print(\"conv1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "#         print(\"conv2 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         print(\"max_pool2d output shape: \", x.shape)\n",
    "        # checking\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x,1)\n",
    "#         print(\"flatten output shape: \", x.shape)\n",
    "        x = self.fc1(x)\n",
    "#         print(\"fc1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "#         print(\"fc2 output shape: \", x.shape)\n",
    "        \n",
    "        output  = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2649, -2.3313, -2.2577, -2.3544, -2.2555, -2.2764, -2.3034, -2.2957,\n",
      "         -2.3005, -2.3953]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "result = net(random_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input shape:  torch.Size([1, 1, 28, 28])\n",
    "\n",
    "conv1 output shape:  torch.Size([1, 32, 26, 26])\n",
    "\n",
    "conv2 output shape:  torch.Size([1, 64, 24, 24])\n",
    "\n",
    "max_pool2d output shape:  torch.Size([1, 64, 12, 12])\n",
    "\n",
    "flatten output shape:  torch.Size([1, 9216])\n",
    "\n",
    "fc1 output shape:  torch.Size([1, 128])\n",
    "\n",
    "fc2 output shape:  torch.Size([1, 10])\n",
    "\n",
    "tensor([[-2.2794, -2.2405, -2.3110, -2.2513, -2.3883, -2.2817, -2.3586, -2.2711,\n",
    "         -2.4489, -2.2183]], grad_fn=<LogSoftmaxBackward>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### param of Conv2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "* groups: 分组卷积 ([Blog](https://www.cnblogs.com/shine-lee/p/10243114.html))\n",
    "* padding_mode: padding模式([Blog](https://blog.csdn.net/hyk_1996/article/details/94447302))\n",
    "\n",
    "### param of MaxPool2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长, 默认None, 即和kernel_size相同\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "\n",
    "卷积/池化输出尺寸:\n",
    "```python\n",
    "import math\n",
    "\n",
    "in_w, in_h =28, 28\n",
    "\n",
    "pad_h, pad_w = 0, 0\n",
    "dil_h, dil_w = 1, 1 # default is 1\n",
    "s_h, s_w = 1, 1 # default is 0\n",
    "k_h, k_w = 3, 3\n",
    "\n",
    "#output featuremap size height\n",
    "out_h = math.floor( (in_h + 2*pad_h - dil_h*(k_h-1) - 1)/s_h + 1)\n",
    "#output featuremap size width\n",
    "out_w = math.floor( (in_w + 2*pad_w - dil_w*(k_w-1) - 1)/s_w + 1)\n",
    "print(out_h)\n",
    "print(out_w)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimize function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader\n",
    "# since we are using MNIST from torchvision, no need for \"class Dataset()\"\n",
    "\n",
    "# transforms\n",
    "\n",
    "#PIL Image Transform\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)),\"output_size not int or tuple\"\n",
    "        self.output_size = output_size\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        if isinstance(self.output_size, int):\n",
    "            new_h = new_w = self.output_size\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        image = image.resize((new_w, new_h))\n",
    "\n",
    "        return image\n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, max_pixle):\n",
    "        self.max_pixle = max_pixle\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.array(image)/self.max_pixle\n",
    "        \n",
    "        return image\n",
    "class ToTensor():\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.expand_dims(image,2)\n",
    "        # H,W,C ==> C,H,W\n",
    "        image = image.transpose((2,0,1))\n",
    "        \n",
    "        image = torch.from_numpy(image).type(torch.FloatTensor)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "# Label Transform\n",
    "class LabelTransform():\n",
    "    def __call__(self, label):\n",
    "        label_arr = np.zeros(10, dtype=int)\n",
    "        label_arr[label] = 1\n",
    "        label = torch.from_numpy(label_arr)\n",
    "#         label = torch.from_numpy(np.array([int(label)]))\n",
    "        return label\n",
    "\n",
    "# dataloader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "compose = transforms.Compose([Rescale(28),Normalize(255),ToTensor()])\n",
    "\n",
    "train = MNIST(\"./data/\", train=True, \\\n",
    "              download=True, transform=compose, \\\n",
    "#              target_transform=LabelTransform()\\\n",
    "             ) \n",
    "test = MNIST(\"./data/\", train=False, \\\n",
    "              download=True, transform=compose, \\\n",
    "#              target_transform=LabelTransform()\\\n",
    "            ) \n",
    "trainloader = DataLoader(train, batch_size = 8, shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(test, batch_size = 8, shuffle =True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 1, 8, 7, 8, 3, 3, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect one mini-batch\n",
    "next(iter(trainloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.742\n",
      "[1,  4000] loss: 0.350\n",
      "[1,  6000] loss: 0.258\n",
      "[2,  2000] loss: 0.158\n",
      "[2,  4000] loss: 0.142\n",
      "[2,  6000] loss: 0.130\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "        labels = labels.to(device)\n",
    "#         labels = labels.type(torch.FloatTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs.shape)\n",
    "#         print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './pytorch_convNet.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  4\n",
      "size:  (28, 28)\n",
      "max pixle value:  255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = MNIST(\"./data/\", train=True, download=True) #60000\n",
    "test= MNIST(\"./data/\",train=False, download=True) #10000\n",
    "\n",
    "# show random train sample\n",
    "\n",
    "plt.imshow(test[500][0], cmap='gray')\n",
    "print(\"label: \", train[num][1])\n",
    "\n",
    "print(\"size: \", train[num][0].size)\n",
    "\n",
    "print(\"max pixle value: \", np.max(train[num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = compose(test[500][0]).unsqueeze(0).to(device)\n",
    "ouputs = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = torch.max(ouputs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4663e-05], device='cuda:0', grad_fn=<MaxBackward0>) tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(_,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "\n",
    "torch.onnx.export(net, dummy_input, \"pytorch_convNet.onnx\",\\\n",
    "                  input_names =[\"input\"],\\\n",
    "                  output_names = [\"output\"],\\\n",
    "                  keep_initializers_as_inputs = True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx.utils\n",
    "\n",
    "\n",
    "model = onnx.load('pytorch_convNet.onnx')\n",
    "polished_model = onnx.utils.polish_model(model)\n",
    "onnx.save(polished_model, \"pytorch_convNet-polished.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "\n",
    "model = onnx.load(\"pytorch_convNet.onnx\")\n",
    "model = onnx.shape_inference.infer_shapes(model)\n",
    "model_graph = model.graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims: 32\n",
      "data_type: 1\n",
      "name: \"conv1.bias\"\n",
      "raw_data: \"\\'\\311\\211>p\\364\\333>\\211\\336\\333=\\3737\\215\\275\\335\\177\\020\\276^\\323\\256<\\034\\3618\\274A\\233Z\\274\\226\\223\\214\\273\\370\\312\\341\\275+Q6\\273\\357\\236\\355\\275\\307P\\010\\276\\266w\\224\\276\\342\\030\\364>\\0345\\277=5\\370\\206=\\354y!\\275\\262y\\227>c\\261\\212\\272y}\\307\\270\\255k\\010>i\\252Z\\276s\\311\\016>\\014X^\\275B\\375\\r\\276\\2049\\245>\\374\\256\\313>\\000+\\220\\274(\\361\\252\\276\\367\\274\\241\\276\\245`K\\276\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in model_graph.initializer:\n",
    "    print(t)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
