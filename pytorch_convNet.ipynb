{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minist dataset: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST(\"./data/\", train=True, download=True) #60000\n",
    "test= MNIST(\"./data/\",train=False, download=True) #10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  3\n",
      "size:  (28, 28)\n",
      "max pixle value:  255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show random train sample\n",
    "num = np.random.randint(len(train))\n",
    "\n",
    "plt.imshow(train[num][0], cmap='gray')\n",
    "print(\"label: \", train[num][1])\n",
    "\n",
    "print(\"size: \", train[num][0].size)\n",
    "\n",
    "print(\"max pixle value: \", np.max(train[num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define net\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "    \n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        # forward process\n",
    "#         print(\"input shape: \", x.shape)\n",
    "        x = self.conv1(x)\n",
    "#         print(\"conv1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "#         print(\"conv2 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         print(\"max_pool2d output shape: \", x.shape)\n",
    "        # checking\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x,1)\n",
    "#         print(\"flatten output shape: \", x.shape)\n",
    "        x = self.fc1(x)\n",
    "#         print(\"fc1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "#         print(\"fc2 output shape: \", x.shape)\n",
    "        \n",
    "        output  = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4296, -2.2800, -2.3422, -2.3414, -2.3885, -2.1033, -2.0418, -2.3675,\n",
      "         -2.4818, -2.3389]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "result = net(random_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input shape:  torch.Size([1, 1, 28, 28])\n",
    "\n",
    "conv1 output shape:  torch.Size([1, 32, 26, 26])\n",
    "\n",
    "conv2 output shape:  torch.Size([1, 64, 24, 24])\n",
    "\n",
    "max_pool2d output shape:  torch.Size([1, 64, 12, 12])\n",
    "\n",
    "flatten output shape:  torch.Size([1, 9216])\n",
    "\n",
    "fc1 output shape:  torch.Size([1, 128])\n",
    "\n",
    "fc2 output shape:  torch.Size([1, 10])\n",
    "\n",
    "tensor([[-2.2794, -2.2405, -2.3110, -2.2513, -2.3883, -2.2817, -2.3586, -2.2711,\n",
    "         -2.4489, -2.2183]], grad_fn=<LogSoftmaxBackward>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### param of Conv2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "* groups: 分组卷积 ([Blog](https://www.cnblogs.com/shine-lee/p/10243114.html))\n",
    "* padding_mode: padding模式([Blog](https://blog.csdn.net/hyk_1996/article/details/94447302))\n",
    "\n",
    "### param of MaxPool2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长, 默认None, 即和kernel_size相同\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "\n",
    "卷积/池化输出尺寸:\n",
    "```python\n",
    "import math\n",
    "\n",
    "in_w, in_h =28, 28\n",
    "\n",
    "pad_h, pad_w = 0, 0\n",
    "dil_h, dil_w = 1, 1 # default is 1\n",
    "s_h, s_w = 1, 1 # default is 0\n",
    "k_h, k_w = 3, 3\n",
    "\n",
    "#output featuremap size height\n",
    "out_h = math.floor( (in_h + 2*pad_h - dil_h*(k_h-1) - 1)/s_h + 1)\n",
    "#output featuremap size width\n",
    "out_w = math.floor( (in_w + 2*pad_w - dil_w*(k_w-1) - 1)/s_w + 1)\n",
    "print(out_h)\n",
    "print(out_w)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimize function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader\n",
    "# since we are using MNIST from torchvision, no need for \"class Dataset()\"\n",
    "\n",
    "# transforms\n",
    "\n",
    "#PIL Image Transform\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)),\"output_size not int or tuple\"\n",
    "        self.output_size = output_size\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        if isinstance(self.output_size, int):\n",
    "            new_h = new_w = self.output_size\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        image = image.resize((new_w, new_h))\n",
    "\n",
    "        return image\n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, max_pixle):\n",
    "        self.max_pixle = max_pixle\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.array(image)/self.max_pixle\n",
    "        \n",
    "        return image\n",
    "class ToTensor():\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.expand_dims(image,2)\n",
    "        # H,W,C ==> C,H,W\n",
    "        image = image.transpose((2,0,1))\n",
    "        \n",
    "        image = torch.from_numpy(image).type(torch.FloatTensor)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "# Label Transform\n",
    "class LabelTransform():\n",
    "    def __call__(self, label):\n",
    "        label_arr = np.zeros(10, dtype=int)\n",
    "        label_arr[label] = 1\n",
    "        label = torch.from_numpy(label_arr)\n",
    "#         label = torch.from_numpy(np.array([int(label)]))\n",
    "        return label\n",
    "\n",
    "# dataloader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "compose = transforms.Compose([Rescale(28),Normalize(255),ToTensor()])\n",
    "\n",
    "train = MNIST(\"./data/\", train=True, \\\n",
    "              download=True, transform=compose, \\\n",
    "#              target_transform=LabelTransform()\\\n",
    "             ) \n",
    "test = MNIST(\"./data/\", train=False, \\\n",
    "              download=True, transform=compose, \\\n",
    "#              target_transform=LabelTransform()\\\n",
    "            ) \n",
    "trainloader = DataLoader(train, batch_size = 8, shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(test, batch_size = 8, shuffle =True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 5, 9, 5, 9, 0, 5, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect one mini-batch\n",
    "next(iter(trainloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.766\n",
      "[1,  4000] loss: 0.380\n",
      "[1,  6000] loss: 0.305\n",
      "[2,  2000] loss: 0.189\n",
      "[2,  4000] loss: 0.160\n",
      "[2,  6000] loss: 0.146\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "        labels = labels.to(device)\n",
    "#         labels = labels.type(torch.FloatTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs.shape)\n",
    "#         print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './pytorch_convNet.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  3\n",
      "size:  (28, 28)\n",
      "max pixle value:  255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = MNIST(\"./data/\", train=True, download=True) #60000\n",
    "test= MNIST(\"./data/\",train=False, download=True) #10000\n",
    "\n",
    "# show random train sample\n",
    "\n",
    "plt.imshow(test[500][0], cmap='gray')\n",
    "print(\"label: \", train[num][1])\n",
    "\n",
    "print(\"size: \", train[num][0].size)\n",
    "\n",
    "print(\"max pixle value: \", np.max(train[num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = compose(test[500][0]).unsqueeze(0).to(device)\n",
    "ouputs = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = torch.max(ouputs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], device='cuda:0', grad_fn=<MaxBackward0>) tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(_,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "\n",
    "torch.onnx.export(net, dummy_input, \"pytorch_convNet.onnx\",\\\n",
    "                  input_names =[\"input\"],\\\n",
    "                  output_names = [\"output\"],\\\n",
    "#                   keep_initializers_as_inputs = True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Input conv1.weight is undefined!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b4d206e1049e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch_convNet.onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpolished_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolish_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolished_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pytorch_convNet-polished.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/onnx/utils.py\u001b[0m in \u001b[0;36mpolish_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_inference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/onnx/optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(model, passes, fixed_point)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moptimized_model_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_fixedpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0moptimized_model_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimized_model_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Input conv1.weight is undefined!"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.utils\n",
    "\n",
    "\n",
    "model = onnx.load('pytorch_convNet.onnx')\n",
    "polished_model = onnx.utils.polish_model(model)\n",
    "onnx.save(polished_model, \"pytorch_convNet-polished.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "\n",
    "model = onnx.load(\"pytorch_convNet.onnx\")\n",
    "model = onnx.shape_inference.infer_shapes(model)\n",
    "model_graph = model.graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims: 32\n",
      "data_type: 1\n",
      "name: \"conv1.bias\"\n",
      "raw_data: \"\\276\\t\\004>\\324n\\325<\\027\\314\\331\\273P\\016\\022>\\346\\013&?\\343\\355t\\2744\\306\\272\\275\\3408]\\276\\221o8\\276X\\222\\205>=\\020\\007?\\304\\234|\\274tj\\031\\272\\320.\\024\\275C\\343\\346<\\223\\325\\025\\276JdK\\274Q\\311\\202\\276{\\000\\303<,\\252\\243\\274=K\\266>E\\207]\\276\\251\\272\\255>\\033\\204|>s\\030\\270\\275z\\335\\327\\275\\332\\307#>\\312\\\"\\352\\274\\212\\352?\\273\\263\\231\\246>,\\263\\034>\\216\\303i\\276\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in model_graph.initializer:\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.array([1,2,3,4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tgc/github/sandbox\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression cannot contain assignment, perhaps you meant \"==\"? (<ipython-input-28-272d10101952>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-272d10101952>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dict(1=2)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expression cannot contain assignment, perhaps you meant \"==\"?\n"
     ]
    }
   ],
   "source": [
    "dict(1=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(shape=a).get('ntop',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<method 'get' of 'dict' objects>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
