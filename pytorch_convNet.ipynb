{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minist dataset: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST(\"./data/\", train=True, download=True) #60000\n",
    "test= MNIST(\"./data/\",train=False, download=True) #10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  8\n",
      "size:  (28, 28)\n",
      "max pixle value:  255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show random train sample\n",
    "num = np.random.randint(len(train))\n",
    "\n",
    "plt.imshow(train[num][0], cmap='gray')\n",
    "print(\"label: \", train[num][1])\n",
    "\n",
    "print(\"size: \", train[num][0].size)\n",
    "\n",
    "print(\"max pixle value: \", np.max(train[num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define net\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "    \n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        # forward process\n",
    "#         print(\"input shape: \", x.shape)\n",
    "        x = self.conv1(x)\n",
    "#         print(\"conv1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "#         print(\"conv2 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         print(\"max_pool2d output shape: \", x.shape)\n",
    "        # checking\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x,1)\n",
    "#         print(\"flatten output shape: \", x.shape)\n",
    "        x = self.fc1(x)\n",
    "#         print(\"fc1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "#         print(\"fc2 output shape: \", x.shape)\n",
    "        \n",
    "        output  = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2321, -2.2505, -2.4137, -2.1847, -2.3393, -2.3317, -2.3493, -2.2990,\n",
      "         -2.4163, -2.2370]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "result = net(random_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input shape:  torch.Size([1, 1, 28, 28])\n",
    "\n",
    "conv1 output shape:  torch.Size([1, 32, 26, 26])\n",
    "\n",
    "conv2 output shape:  torch.Size([1, 64, 24, 24])\n",
    "\n",
    "max_pool2d output shape:  torch.Size([1, 64, 12, 12])\n",
    "\n",
    "flatten output shape:  torch.Size([1, 9216])\n",
    "\n",
    "fc1 output shape:  torch.Size([1, 128])\n",
    "\n",
    "fc2 output shape:  torch.Size([1, 10])\n",
    "\n",
    "tensor([[-2.2794, -2.2405, -2.3110, -2.2513, -2.3883, -2.2817, -2.3586, -2.2711,\n",
    "         -2.4489, -2.2183]], grad_fn=<LogSoftmaxBackward>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### param of Conv2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "* groups: 分组卷积 ([Blog](https://www.cnblogs.com/shine-lee/p/10243114.html))\n",
    "* padding_mode: padding模式([Blog](https://blog.csdn.net/hyk_1996/article/details/94447302))\n",
    "\n",
    "### param of MaxPool2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长, 默认None, 即和kernel_size相同\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "\n",
    "卷积/池化输出尺寸:\n",
    "```python\n",
    "import math\n",
    "\n",
    "in_w, in_h =28, 28\n",
    "\n",
    "pad_h, pad_w = 0, 0\n",
    "dil_h, dil_w = 1, 1 # default is 1\n",
    "s_h, s_w = 1, 1 # default is 0\n",
    "k_h, k_w = 3, 3\n",
    "\n",
    "#output featuremap size height\n",
    "out_h = math.floor( (in_h + 2*pad_h - dil_h*(k_h-1) - 1)/s_h + 1)\n",
    "#output featuremap size width\n",
    "out_w = math.floor( (in_w + 2*pad_w - dil_w*(k_w-1) - 1)/s_w + 1)\n",
    "print(out_h)\n",
    "print(out_w)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimize function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader\n",
    "# since we are using MNIST from torchvision, no need for \"class Dataset()\"\n",
    "\n",
    "# transforms\n",
    "\n",
    "#PIL Image Transform\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)),\"output_size not int or tuple\"\n",
    "        self.output_size = output_size\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        if isinstance(self.output_size, int):\n",
    "            new_h = new_w = self.output_size\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        image = image.resize((new_w, new_h))\n",
    "\n",
    "        return image\n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, max_pixle):\n",
    "        self.max_pixle = max_pixle\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.array(image)/self.max_pixle\n",
    "        \n",
    "        return image\n",
    "class ToTensor():\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.expand_dims(image,2)\n",
    "        # H,W,C ==> C,H,W\n",
    "        image = image.transpose((2,0,1))\n",
    "        \n",
    "        image = torch.from_numpy(image).type(torch.FloatTensor)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "# Label Transform\n",
    "class LabelTransform():\n",
    "    def __call__(self, label):\n",
    "        label_arr = np.zeros(10, dtype=int)\n",
    "        label_arr[label] = 1\n",
    "        label = torch.from_numpy(label_arr)\n",
    "#         label = torch.from_numpy(np.array([int(label)]))\n",
    "        return label\n",
    "\n",
    "# dataloader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "compose = transforms.Compose([Rescale(28),Normalize(255),ToTensor()])\n",
    "\n",
    "train = MNIST(\"./data/\", train=True, \\\n",
    "              download=True, transform=compose, \\\n",
    "#              target_transform=LabelTransform()\\\n",
    "             ) \n",
    "test = MNIST(\"./data/\", train=False, \\\n",
    "              download=True, transform=compose, \\\n",
    "#              target_transform=LabelTransform()\\\n",
    "            ) \n",
    "trainloader = DataLoader(train, batch_size = 8, shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(test, batch_size = 8, shuffle =True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 3, 7, 6, 2, 5, 0, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect one mini-batch\n",
    "next(iter(trainloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.747\n",
      "[1,  4000] loss: 0.373\n",
      "[1,  6000] loss: 0.287\n",
      "[2,  2000] loss: 0.183\n",
      "[2,  4000] loss: 0.147\n",
      "[2,  6000] loss: 0.140\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "        labels = labels.to(device)\n",
    "#         labels = labels.type(torch.FloatTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs.shape)\n",
    "#         print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './pytorch_convNet.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  8\n",
      "size:  (28, 28)\n",
      "max pixle value:  255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = MNIST(\"./data/\", train=True, download=True) #60000\n",
    "test= MNIST(\"./data/\",train=False, download=True) #10000\n",
    "\n",
    "# show random train sample\n",
    "\n",
    "plt.imshow(test[500][0], cmap='gray')\n",
    "print(\"label: \", train[num][1])\n",
    "\n",
    "print(\"size: \", train[num][0].size)\n",
    "\n",
    "print(\"max pixle value: \", np.max(train[num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = compose(test[500][0]).unsqueeze(0).to(device)\n",
    "ouputs = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = torch.max(ouputs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1206e-05], device='cuda:0', grad_fn=<MaxBackward0>) tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(_,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "\n",
    "torch.onnx.export(net, dummy_input, \"pytorch_convNet.onnx\",\\\n",
    "                  input_names =[\"input\"],\\\n",
    "                  output_names = [\"output\"],\\\n",
    "                  keep_initializers_as_inputs = True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b4d206e1049e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch_convNet.onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.utils\n",
    "\n",
    "\n",
    "model = onnx.load('pytorch_convNet.onnx')\n",
    "polished_model = onnx.utils.polish_model(model)\n",
    "onnx.save(polished_model, \"pytorch_convNet-polished.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "\n",
    "model = onnx.load(\"pytorch_convNet.onnx\")\n",
    "model = onnx.shape_inference.infer_shapes(model)\n",
    "model_graph = model.graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in model_graph.initializer:\n",
    "    print(t)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
