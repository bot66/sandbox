{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minist dataset: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST(\"./data/\", train=True, download=True) #60000\n",
    "test= MNIST(\"./data/\",train=False, download=True) #10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  2\n",
      "size:  (28, 28)\n",
      "max pixle value:  255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show random train sample\n",
    "num = np.random.randint(len(train))\n",
    "\n",
    "plt.imshow(train[num][0], cmap='gray')\n",
    "print(\"label: \", train[num][1])\n",
    "\n",
    "print(\"size: \", train[num][0].size)\n",
    "\n",
    "print(\"max pixle value: \", np.max(train[num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define net\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "    \n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        # forward process\n",
    "#         print(\"input shape: \", x.shape)\n",
    "        x = self.conv1(x)\n",
    "#         print(\"conv1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "#         print(\"conv2 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         print(\"max_pool2d output shape: \", x.shape)\n",
    "        # checking\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x,1)\n",
    "#         print(\"flatten output shape: \", x.shape)\n",
    "        x = self.fc1(x)\n",
    "#         print(\"fc1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "#         print(\"fc2 output shape: \", x.shape)\n",
    "        \n",
    "        output  = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2649, -2.3313, -2.2577, -2.3544, -2.2555, -2.2764, -2.3034, -2.2957,\n",
      "         -2.3005, -2.3953]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "result = net(random_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input shape:  torch.Size([1, 1, 28, 28])\n",
    "\n",
    "conv1 output shape:  torch.Size([1, 32, 26, 26])\n",
    "\n",
    "conv2 output shape:  torch.Size([1, 64, 24, 24])\n",
    "\n",
    "max_pool2d output shape:  torch.Size([1, 64, 12, 12])\n",
    "\n",
    "flatten output shape:  torch.Size([1, 9216])\n",
    "\n",
    "fc1 output shape:  torch.Size([1, 128])\n",
    "\n",
    "fc2 output shape:  torch.Size([1, 10])\n",
    "\n",
    "tensor([[-2.2794, -2.2405, -2.3110, -2.2513, -2.3883, -2.2817, -2.3586, -2.2711,\n",
    "         -2.4489, -2.2183]], grad_fn=<LogSoftmaxBackward>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### param of Conv2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "* groups: 分组卷积 ([Blog](https://www.cnblogs.com/shine-lee/p/10243114.html))\n",
    "* padding_mode: padding模式([Blog](https://blog.csdn.net/hyk_1996/article/details/94447302))\n",
    "\n",
    "### param of MaxPool2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长, 默认None, 即和kernel_size相同\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "\n",
    "卷积/池化输出尺寸:\n",
    "```python\n",
    "import math\n",
    "\n",
    "in_w, in_h =28, 28\n",
    "\n",
    "pad_h, pad_w = 0, 0\n",
    "dil_h, dil_w = 1, 1 # default is 1\n",
    "s_h, s_w = 1, 1 # default is 0\n",
    "k_h, k_w = 3, 3\n",
    "\n",
    "#output featuremap size height\n",
    "out_h = math.floor( (in_h + 2*pad_h - dil_h*(k_h-1) - 1)/s_h + 1)\n",
    "#output featuremap size width\n",
    "out_w = math.floor( (in_w + 2*pad_w - dil_w*(k_w-1) - 1)/s_w + 1)\n",
    "print(out_h)\n",
    "print(out_w)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimize function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader\n",
    "# since we are using MNIST from torchvision, no need for \"class Dataset()\"\n",
    "\n",
    "# transforms\n",
    "\n",
    "#PIL Image Transform\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)),\"output_size not int or tuple\"\n",
    "        self.output_size = output_size\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        if isinstance(self.output_size, int):\n",
    "            new_h = new_w = self.output_size\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        image = image.resize((new_w, new_h))\n",
    "\n",
    "        return image\n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, max_pixle):\n",
    "        self.max_pixle = max_pixle\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.array(image)/self.max_pixle\n",
    "        \n",
    "        return image\n",
    "class ToTensor():\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.expand_dims(image,2)\n",
    "        # H,W,C ==> C,H,W\n",
    "        image = image.transpose((2,0,1))\n",
    "        \n",
    "        image = torch.from_numpy(image).type(torch.FloatTensor)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "# Label Transform\n",
    "class LabelTransform():\n",
    "    def __call__(self, label):\n",
    "        label_arr = np.zeros(10, dtype=int)\n",
    "        label_arr[label] = 1\n",
    "        label = torch.from_numpy(label_arr)\n",
    "#         label = torch.from_numpy(np.array([int(label)]))\n",
    "        return label\n",
    "\n",
    "# dataloader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "compose = transforms.Compose([Rescale(28),Normalize(255),ToTensor()])\n",
    "\n",
    "train = MNIST(\"./data/\", train=True, \\\n",
    "              download=True, transform=compose, \\\n",
    "#              target_transform=LabelTransform()\\\n",
    "             ) \n",
    "test = MNIST(\"./data/\", train=False, \\\n",
    "              download=True, transform=compose, \\\n",
    "#              target_transform=LabelTransform()\\\n",
    "            ) \n",
    "trainloader = DataLoader(train, batch_size = 8, shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(test, batch_size = 8, shuffle =True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 1, 8, 7, 8, 3, 3, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect one mini-batch\n",
    "next(iter(trainloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.742\n",
      "[1,  4000] loss: 0.350\n",
      "[1,  6000] loss: 0.258\n",
      "[2,  2000] loss: 0.158\n",
      "[2,  4000] loss: 0.142\n",
      "[2,  6000] loss: 0.130\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "        labels = labels.to(device)\n",
    "#         labels = labels.type(torch.FloatTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs.shape)\n",
    "#         print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './pytorch_convNet.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  4\n",
      "size:  (28, 28)\n",
      "max pixle value:  255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = MNIST(\"./data/\", train=True, download=True) #60000\n",
    "test= MNIST(\"./data/\",train=False, download=True) #10000\n",
    "\n",
    "# show random train sample\n",
    "\n",
    "plt.imshow(test[500][0], cmap='gray')\n",
    "print(\"label: \", train[num][1])\n",
    "\n",
    "print(\"size: \", train[num][0].size)\n",
    "\n",
    "print(\"max pixle value: \", np.max(train[num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = compose(test[500][0]).unsqueeze(0).to(device)\n",
    "ouputs = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = torch.max(ouputs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4663e-05], device='cuda:0', grad_fn=<MaxBackward0>) tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(_,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "\n",
    "torch.onnx.export(net, dummy_input, \"pytorch_convNet.onnx\",\\\n",
    "                  input_names =[\"input\"],\\\n",
    "                  output_names = [\"output\"],\\\n",
    "                  keep_initializers_as_inputs = True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx.utils\n",
    "\n",
    "\n",
    "model = onnx.load('pytorch_convNet.onnx')\n",
    "polished_model = onnx.utils.polish_model(model)\n",
    "onnx.save(polished_model, \"pytorch_convNet-polished.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "\n",
    "model = onnx.load(\"pytorch_convNet.onnx\")\n",
    "model = onnx.shape_inference.infer_shapes(model)\n",
    "model_graph = model.graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims: 32\n",
      "data_type: 1\n",
      "name: \"conv1.bias\"\n",
      "raw_data: \"\\'\\311\\211>p\\364\\333>\\211\\336\\333=\\3737\\215\\275\\335\\177\\020\\276^\\323\\256<\\034\\3618\\274A\\233Z\\274\\226\\223\\214\\273\\370\\312\\341\\275+Q6\\273\\357\\236\\355\\275\\307P\\010\\276\\266w\\224\\276\\342\\030\\364>\\0345\\277=5\\370\\206=\\354y!\\275\\262y\\227>c\\261\\212\\272y}\\307\\270\\255k\\010>i\\252Z\\276s\\311\\016>\\014X^\\275B\\375\\r\\276\\2049\\245>\\374\\256\\313>\\000+\\220\\274(\\361\\252\\276\\367\\274\\241\\276\\245`K\\276\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in model_graph.initializer:\n",
    "    print(t)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
