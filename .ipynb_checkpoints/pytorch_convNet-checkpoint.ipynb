{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minist dataset: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST(\"./data/\", train=True, download=True) #60000\n",
    "test= MNIST(\"./data/\",train=False, download=True) #10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  3\n",
      "size:  (28, 28)\n",
      "max pixle value:  255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show random train sample\n",
    "num = np.random.randint(len(train))\n",
    "\n",
    "plt.imshow(train[num][0], cmap='gray')\n",
    "print(\"label: \", train[num][1])\n",
    "\n",
    "print(\"size: \", train[num][0].size)\n",
    "\n",
    "print(\"max pixle value: \", np.max(train[num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define net\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "    \n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        # forward process\n",
    "        print(\"input shape: \", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        print(\"conv1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        print(\"conv2 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        print(\"max_pool2d output shape: \", x.shape)\n",
    "        # checking\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        print(\"flatten output shape: \", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        print(\"fc1 output shape: \", x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        print(\"fc2 output shape: \", x.shape)\n",
    "        \n",
    "        output  = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([1, 1, 28, 28])\n",
      "conv1 output shape:  torch.Size([1, 32, 26, 26])\n",
      "conv2 output shape:  torch.Size([1, 64, 24, 24])\n",
      "max_pool2d output shape:  torch.Size([1, 64, 12, 12])\n",
      "flatten output shape:  torch.Size([1, 9216])\n",
      "fc1 output shape:  torch.Size([1, 128])\n",
      "fc2 output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "net = Net()\n",
    "result = net(random_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### param of Conv2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "* groups: 分组卷积 ([Blog](https://www.cnblogs.com/shine-lee/p/10243114.html))\n",
    "* padding_mode: padding模式([Blog](https://blog.csdn.net/hyk_1996/article/details/94447302))\n",
    "\n",
    "### param of MaxPool2d\n",
    "(height, width)\n",
    "* stride： 卷积核移动步长, 默认None, 即和kernel_size相同\n",
    "* padding： padding size\n",
    "* dilation: 感受野散开程度 （[可视化](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)）\n",
    "\n",
    "卷积/池化输出尺寸:\n",
    "```python\n",
    "import math\n",
    "\n",
    "in_w, in_h =28, 28\n",
    "\n",
    "pad_h, pad_w = 0, 0\n",
    "dil_h, dil_w = 1, 1 # default is 1\n",
    "s_h, s_w = 1, 1 # default is 0\n",
    "k_h, k_w = 3, 3\n",
    "\n",
    "#output featuremap size height\n",
    "out_h = math.floor( (in_h + 2*pad_h - dil_h*(k_h-1) - 1)/s_h + 1)\n",
    "#output featuremap size width\n",
    "out_w = math.floor( (in_w + 2*pad_w - dil_w*(k_w-1) - 1)/s_w + 1)\n",
    "print(out_h)\n",
    "print(out_w)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimize function\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader\n",
    "# since we are using MNIST from torchvision, no need for \"class Dataset()\"\n",
    "\n",
    "# transforms\n",
    "\n",
    "#PIL Image Transform\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)),\"output_size not int or tuple\"\n",
    "        self.output_size = output_size\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        if isinstance(self.output_size, int):\n",
    "            new_h = new_w = self.output_size\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        image = image.resize((new_w, new_h))\n",
    "\n",
    "        return image\n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, max_pixle):\n",
    "        self.max_pixle = max_pixle\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.array(image)/self.max_pixle\n",
    "        \n",
    "        return image\n",
    "class ToTensor():\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = np.expand_dims(image,2)\n",
    "        # H,W,C ==> C,H,W\n",
    "        image = image.transpose((2,0,1))\n",
    "        \n",
    "        image = torch.from_numpy(image)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "# Label Transform\n",
    "class LabelTransform():\n",
    "    def __call__(self, label):\n",
    "        label_arr = np.zeros(10, dtype=int)\n",
    "        label_arr[label] = 1\n",
    "        label = torch.from_numpy(label_arr)\n",
    "        return label\n",
    "\n",
    "# dataloader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "compose = transforms.Compose([Rescale(28),Normalize(255),ToTensor()])\n",
    "\n",
    "train = MNIST(\"./data/\", train=True, \\\n",
    "              download=True, transform=compose, \\\n",
    "              target_transform=LabelTransform()) #60000\n",
    "train_data = DataLoader(train, batch_size = 8, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
